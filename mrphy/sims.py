r"""Simulation codes with explicit Jacobian operations.
"""

from typing import Tuple, Optional

import torch
from torch import Tensor
from torch.autograd import Function
from torch.autograd.function import _ContextMethodMixin as CTX

from mrphy import Œ≥H, dt0, œÄ


# TODO:
# - Avoid caching when needs_input_grad is False
# - Create `BlochSim_rfgr` that directly computes grads w.r.t. `rf` and `gr`.


__all__ = ['blochsim']

_contiguous_format = torch.contiguous_format


class BlochSim(Function):
    r"""BlochSim with explict Jacobian operation (backward)

    This operator is only differentiable w.r.t. ``Mi`` and ``Beff``.

    """

    @staticmethod
    def forward(
        ctx: CTX,
        Mi: Tensor,
        Beff: Tensor,
        T1: Optional[Tensor],
        T2: Optional[Tensor],
        Œ≥: Tensor,
        dt: Tensor
    ) -> Tensor:
        r"""Forward evolution of Bloch simulation

        Inputs:
            - ``ctx``: `(1,)`, pytorch CTX cacheing object
            - ``Mi``: `(N, *Nd, xyz)`, Magnetic spins, assumed equilibrium \
              [0 0 1]
            - ``Beff``: `(N, *Nd, nT, xyz)`, "Gauss", B-effective, magnetic \
              field.
            - ``T1``: `(N ‚äª 1, *Nd ‚äª len(Nd)*(1,), 1, 1)`, "Sec", T‚ÇÅ
            - ``T2``: `(N ‚äª 1, *Nd ‚äª len(Nd)*(1,), 1, 1)`, "Sec", T‚ÇÇ
            - ``Œ≥``:  `(N ‚äª 1, *Nd ‚äª len(Nd)*(1,), 1, 1)`, "Hz/Gauss", gyro.
            - ``dt``: `(N ‚äª 1, len(Nd)*(1,), 1, 1)`, "Sec", dwell time.
        Outputs:
            - ``Mo``: `(N, *Nd, xyz)`, Magetic spins after simulation.
        """
        NNd, nT = Beff.shape[:-2], Beff.shape[-2]
        # (t)ensor (k)ey(w)ord, contiguous to avoid alloc/copy when reshape
        tkw = {'memory_format': _contiguous_format,
               'dtype': Mi.dtype, 'device': Mi.device}

        # %% Preprocessing
        Œ≥2œÄdt = 2*œÄ*Œ≥*dt
        Œ≥Beff = torch.empty(Beff.shape, **tkw)
        torch.mul(Œ≥2œÄdt, Beff, out=Œ≥Beff)
        Mi = Mi.clone(memory_format=_contiguous_format)[..., None, :]
        # Œ≥Beff = Œ≥2œÄdt*Beff.contiguous()

        assert((T1 is None) == (T2 is None))  # both or neither

        if T1 is None:  # relaxations ignored
            E = e1_1 = None
            fn_relax_ = lambda m1: None
        else:
            E1, E2 = -dt/T1, -dt/T2
            E1.exp_(), E2.exp_()  # should have fewer alloc than exp(-dt/T1)
            E, e1_1 = torch.cat((E2, E2, E1), dim=-1), E1-1
            fn_relax_ = lambda m1: (m1.mul_(E))[..., 2:3].sub_(e1_1)

        # Pre-allocate intermediate variables, in case of overhead alloc's
        v = torch.empty(Mi.shape, **tkw)  # (N, *Nd, xyz, 1)

        # %% Other variables to be cached
        m0 = Mi
        Mhst = torch.empty(NNd+(nT, 3), **tkw)
        Œ¶ = torch.empty(NNd+(nT, 1), **tkw)
        cŒ¶_1 = torch.empty(NNd+(nT, 1), **tkw)
        sŒ¶ = torch.empty(NNd+(nT, 1), **tkw)
        UtM0 = torch.empty(NNd+(nT, 1), **tkw)

        # %% Simulation. could we learn to live right.
        for m1, Œ≥beff, œï, cœï_1, sœï, utm0 in zip(
            Mhst.split(1, dim=-2),
            Œ≥Beff.split(1, dim=-2),
            Œ¶.split(1, dim=-2),
            cŒ¶_1.split(1, dim=-2),
            sŒ¶.split(1, dim=-2),
            UtM0.split(1, dim=-2),
        ):
            # Rotation
            torch.norm(Œ≥beff, dim=-1, keepdim=True, out=œï)
            œï.clamp_(min=1e-12)
            torch.div(Œ≥beff, œï, out=Œ≥beff)
            u = Œ≥beff

            torch.sin(œï, out=sœï)
            torch.cos(œï, out=cœï_1)
            cœï_1.sub_(1)  # (cœï-1)

            # wiki/Rotation_matrix#Rotation_matrix_from_axis_and_angle
            # Angle is `-œï` as Bloch-eq is ùëÄ√óùêµ
            # m‚ÇÅ = R(u, -œï)m‚ÇÄ = cœï*m‚ÇÄ + (1-cœï)*u·µÄm‚ÇÄ*u - sœï*u√óm‚ÇÄ
            # m‚ÇÅ = m‚ÇÄ - sœï*u√óm‚ÇÄ + (cœï-1)*(m‚ÇÄ - u·µÄm‚ÇÄ*u), in-place friendly
            torch.mul(u, m0, out=m1)  # using m‚ÇÅ as an temporary storage
            torch.sum(m1, dim=-1, keepdim=True, out=utm0)

            torch.cross(u, m0, dim=-1, out=m1)  # u√óm‚ÇÄ
            torch.addcmul(m0, sœï, m1, value=-1, out=m1)  # m‚ÇÄ - sœï*(u√óm‚ÇÄ)

            torch.addcmul(m0, utm0, u, value=-1, out=v)  # m‚ÇÄ - u·µÄm‚ÇÄ*u

            torch.addcmul(m1, cœï_1, v, out=m1)  # m‚ÇÄ-sœï*u√óm‚ÇÄ+(cœï-1)*(m‚ÇÄ-u·µÄm‚ÇÄ*u)

            # Relaxation
            fn_relax_(m1)

            m0 = m1

        ctx.save_for_backward(
            Mi, Mhst, Œ≥Beff, Œ¶, cŒ¶_1, sŒ¶, UtM0, E, e1_1, Œ≥2œÄdt
        )
        Mo = Mhst[..., -1, :].clone()  # -> (N, *Nd, xyz)
        return Mo

    @staticmethod
    def backward(
        ctx: CTX,
        grad_Mo: Tensor
    ) -> Tuple[Tensor, Tensor, None, None, None, None]:
        r"""Backward evolution of Bloch simulation Jacobians

        Inputs:
            - ``ctx``: `(1,)`, pytorch CTX cacheing object
            - ``grad_Mo``: `(N, *Nd, xyz)`, derivative w.r.t. output Magetic \
              spins.
        Outputs:
            - ``grad_Mi``: `(N, *Nd, xyz)`, derivative w.r.t. input Magetic \
              spins.
            - ``grad_Beff``: `(N,*Nd,nT,xyz)`, derivative w.r.t. B-effective.
            - None*4, this implemendation do not provide derivatives w.r.t.: \
              `T1`, `T2`, `Œ≥`, and `dt`.
        """
        # grads of configuration variables are not supported yet
        needs_grad = ctx.needs_input_grad
        grad_Beff = grad_Mi = grad_T1 = grad_T2 = grad_Œ≥ = grad_dt = None

        if not any(needs_grad[0:2]):  # (Mi,Beff;T1,T2,Œ≥,dt):
            return grad_Mi, grad_Beff, grad_T1, grad_T2, grad_Œ≥, grad_dt

        # %% Jacobians. If we turn back time,
        # ctx.save_for_backward(Mi, Mhst, Œ≥Beff, E, e1_1, Œ≥2œÄdt)
        Mi, Mhst, Œ≥Beff, Œ¶, cŒ¶_1, sŒ¶, UtM0, E, e1_1, Œ≥2œÄdt = ctx.saved_tensors
        NNd = Œ≥Beff.shape[:-2]
        # (t)ensor (k)ey(w)ord, contiguous to avoid alloc/copy when reshape
        tkw = {'memory_format': _contiguous_format,
               'dtype': Mi.dtype, 'device': Mi.device}

        # assert((E is None) == (e1_1 is None))  # both or neither
        if E is None:  # relaxations ignored
            fn_relax_h1_ = lambda h1: None
            fn_relax_m1_ = lambda m1: None
        else:
            fn_relax_h1_ = lambda h1: h1.mul_(E)

            def fn_relax_m1_(m1):
                m1[..., :, 2:3].add_(e1_1)
                m1.div_(E)
                return

        # Pre-allocate intermediate variables, in case of overhead alloc's
        h0 = torch.empty(NNd+(1, 3), **tkw)

        u, uxh1 = (torch.empty(NNd+(1, 3), **tkw) for _ in range(2))
        œï, cœï_1, sœï, uth1 = (torch.empty(NNd+(1, 1), **tkw) for _ in range(4))
        # œïis0 = torch.empty(NNd+(1, 1),
        #                    memory_format=tkw['memory_format'],
        #                    device=tkw['device'], dtype=torch.bool)
        h1 = grad_Mo.clone(memory_format=_contiguous_format)[..., None, :]
        # u_dflt = torch.tensor([[0.], [0.], [1.]],  # (xyz, 1)
        #                       device=tkw['device'], dtype=tkw['dtype'])

        m1 = Mhst.narrow(-2, -1, 1)

        # scale by -Œ≥2œÄdt, so output ‚àÇL/‚àÇB no longer needs multiply by -Œ≥2œÄdt
        h1.mul_(-Œ≥2œÄdt)
        for m0, Œ≥beff, œï, cœï_1, sœï, utm0 in zip(
            reversed((Mi,)+Mhst.split(1, dim=-2)[:-1]),
            reversed(Œ≥Beff.split(1, dim=-2)),
            reversed(Œ¶.split(1, dim=-2)),
            reversed(cŒ¶_1.split(1, dim=-2)),
            reversed(sŒ¶.split(1, dim=-2)),
            reversed(UtM0.split(1, dim=-2)),
        ):
            # %% Ajoint Relaxation:
            fn_relax_h1_(h1)  # h‚ÇÅ ‚Üí hÃÉ‚ÇÅ ‚âî ‚àÇL/‚àÇmÃÉ‚ÇÅ = E‚àÇL/‚àÇm‚ÇÅ
            fn_relax_m1_(m1)  # m‚ÇÅ ‚Üí mÃÉ‚ÇÅ ‚âî Rm‚ÇÄ = E‚Åª¬πm‚ÇÅ

            # %% Adjoint Rotations:
            u.copy_(Œ≥beff)

            # TODO: Resolve singularities of œï=0, control pov?
            # torch.logical_not(œï, out=œïis0)
            # if torch.any(œïis0):
            #     u[œïis0[..., 0, 0]] = u_dflt

            # %% Assemble h‚ÇÄ: (R(u, -œï)·µÄ ‚â° R(u, œï))
            # h‚ÇÄ ‚âî R(u, œï)hÃÉ‚ÇÅ = cœï*h‚ÇÅ + (1-cœï)*u·µÄh‚ÇÅ*u + sœï*u√óh‚ÇÅ
            # h‚ÇÄ = hÃÉ‚ÇÅ + (cœï-1)*(hÃÉ‚ÇÅ - u·µÄhÃÉ‚ÇÅ*u) + sœï*u√óhÃÉ‚ÇÅ, in-place friendly
            torch.mul(u, h1, out=h0)  # using h0 as an temporary storage
            torch.sum(h0, dim=-1, keepdim=True, out=uth1)  # u·µÄhÃÉ‚ÇÅ

            torch.cross(u, h1, dim=-1, out=uxh1)  # u√óhÃÉ‚ÇÅ
            torch.addcmul(h1, uth1, u, value=-1, out=h0)  # hÃÉ‚ÇÅ-u·µÄhÃÉ‚ÇÅ*u

            torch.addcmul(h1, cœï_1, h0, out=h0)  # hÃÉ‚ÇÅ + (cœï-1)*(hÃÉ‚ÇÅ-u·µÄhÃÉ‚ÇÅ*u)

            # Finish: h‚ÇÄ = hÃÉ‚ÇÅ + (cœï-1)*(hÃÉ‚ÇÅ - u·µÄhÃÉ‚ÇÅ*u) + sœï*u√óhÃÉ‚ÇÅ
            torch.addcmul(h0, sœï, uxh1, value=1, out=h0)

            # %% Assemble ‚àÇL/‚àÇB[..., t], store into Œ≥beff
            # -Œ≥Œ¥t‚ãÖ(+sœï/œï‚ãÖm‚ÇÄ√óhÃÉ‚ÇÅ
            #       +(cœï-1)/œï‚ãÖ(u·µÄm‚ÇÄ‚ãÖhÃÉ‚ÇÅ+u·µÄhÃÉ‚ÇÅ‚ãÖm‚ÇÄ)
            #       +((sœï/œï‚ãÖm‚ÇÄ-mÃÉ‚ÇÅ)·µÄ(u√óhÃÉ‚ÇÅ)-2(cœï-1)/œï‚ãÖu·µÄm‚ÇÄ‚ãÖu·µÄhÃÉ1)*u )

            cœï_1.div_(œï), sœï.div_(œï)  # cœï-1, sœï ‚Üí (cœï-1)/œï, sœï/œï
            # if torch.any(œïis0):  # handle division-by-0
            #     cœï_1[œïis0], sœï[œïis0] = 0, 1

            # %%% sœï/œï‚ãÖ(m‚ÇÄ√óhÃÉ‚ÇÅ)
            torch.cross(m0, h1, dim=-1, out=Œ≥beff)  # m‚ÇÄ√óhÃÉ‚ÇÅ
            Œ≥beff.mul_(sœï)  # sœï/œï‚ãÖ(m‚ÇÄ√óhÃÉ‚ÇÅ)

            # %%% sœï/œï‚ãÖ(m‚ÇÄ√óhÃÉ‚ÇÅ) + (cœï-1)/œï‚ãÖ(u·µÄm‚ÇÄ‚ãÖhÃÉ‚ÇÅ+u·µÄhÃÉ‚ÇÅ‚ãÖm‚ÇÄ)
            h1.mul_(utm0)  # u·µÄm‚ÇÄ‚ãÖhÃÉ‚ÇÅ
            torch.addcmul(h1, uth1, m0, out=h1)  # (u·µÄm‚ÇÄ‚ãÖhÃÉ‚ÇÅ+u·µÄhÃÉ‚ÇÅ‚ãÖm‚ÇÄ)
            torch.addcmul(Œ≥beff, cœï_1, h1, value=1, out=Œ≥beff)

            # %%% sœï/œï‚ãÖ(m‚ÇÄ√óhÃÉ‚ÇÅ) + (cœï-1)/œï‚ãÖ(u·µÄm‚ÇÄ‚ãÖhÃÉ‚ÇÅ+u·µÄhÃÉ‚ÇÅ‚ãÖm‚ÇÄ)
            #     -((mÃÉ‚ÇÅ-sœï/œï‚ãÖm‚ÇÄ)·µÄ(u√óhÃÉ‚ÇÅ) + 2(cœï-1)/œï‚ãÖu·µÄhÃÉ1‚ãÖu·µÄm‚ÇÄ)‚ãÖu

            # (mÃÉ‚ÇÅ-sœï/œï‚ãÖm‚ÇÄ)·µÄ(u√óhÃÉ‚ÇÅ)
            torch.addcmul(m1, sœï, m0, value=-1, out=m1)  # (mÃÉ‚ÇÅ-sœï/œï‚ãÖm‚ÇÄ)
            m1.mul_(uxh1)
            torch.sum(m1, dim=-1, keepdim=True, out=sœï)

            # ((mÃÉ‚ÇÅ-sœï/œï‚ãÖm‚ÇÄ)·µÄ(u√óhÃÉ‚ÇÅ) + 2(cœï-1)/œï‚ãÖu·µÄhÃÉ‚ÇÅ‚ãÖu·µÄm‚ÇÄ)
            uth1.mul_(utm0)  # u·µÄhÃÉ1‚ãÖu·µÄm‚ÇÄ
            torch.addcmul(sœï, cœï_1, uth1, value=2, out=cœï_1)

            torch.addcmul(Œ≥beff, cœï_1, u, value=-1, out=Œ≥beff)

            m1, h1, h0 = m0, h0, h1

        # %% Clean up
        grad_Beff = Œ≥Beff

        # undo the multiply by -Œ≥2œÄdt on h1
        grad_Mi = h1[..., 0, :].div_(-Œ≥2œÄdt[0, ...]) if needs_grad[0] else None
        # forward(ctx, Mi, Beff; T1, T2, Œ≥, dt):
        return grad_Mi, grad_Beff, grad_T1, grad_T2, grad_Œ≥, grad_dt


def blochsim(
    Mi: Tensor, Beff: Tensor, *,
    T1: Optional[Tensor] = None, T2: Optional[Tensor] = None,
    Œ≥: Tensor = Œ≥H, dt: Tensor = dt0
) -> Tensor:
    r"""Bloch simulator with explicit Jacobian operation.

    This function is only differentiable w.r.t. ``Mi`` and ``Beff``.

    Setting `T1=T2=None` to opt for simulation ignoring relaxation.

    Usage:
        ``Mo = blochsim(Mi, Beff, *, T1, T2, Œ≥, dt)``
        ``Mo = blochsim(Mi, Beff, *, T1=None, T2=None, Œ≥, dt)``
    Inputs:
        - ``Mi``: `(N, *Nd, xyz)`, Magnetic spins, assumed equilibrium \
          [[[0 0 1]]].
        - ``Beff``: `(N, *Nd, nT, xyz)`, "Gauss", B-effective, magnetic field.
    Optionals:
        - ``T1``: `()` ‚äª `(N ‚äª 1, *Nd ‚äª 1,)`, "Sec", T1 relaxation.
        - ``T2``: `()` ‚äª `(N ‚äª 1, *Nd ‚äª 1,)`, "Sec", T2 relaxation.
        - ``Œ≥``:  `()` ‚äª `(N ‚äª 1, *Nd ‚äª 1,)`, "Hz/Gauss", gyro ratio.
        - ``dt``: `()` ‚äª `(N ‚äª 1,)`, "Sec", dwell time.
    Outputs:
        - ``Mo``: `(N, *Nd, xyz)`, Magetic spins after simulation.

    .. tip::
        For alternative implementation:
        Storing history for `U`, `Œ¶` and `UtM0` etc., which are also used in
        `backward`, may avoid redundant computation, but comsumes more RAM.
    """

    # %% Defaults and move to the same device
    assert(Mi.shape[:-1] == Beff.shape[:-2])
    Beff, ndim = Beff.to(Mi.device), Beff.ndim

    # Make {Œ≥, dt, T1, T2} compatible with (N, *Nd, :, :)
    Œ≥, dt = (x.reshape(x.shape+(ndim-x.ndim)*(1,)) for x in (Œ≥, dt))

    assert((T1 is None) == (T2 is None))  # both or neither
    if T1 is not None:
        T1, T2 = (x.reshape(x.shape+(ndim-x.ndim)*(1,)) for x in (T1, T2))

    return BlochSim.apply(Mi, Beff, T1, T2, Œ≥, dt)


class FreePrec(Function):
    r"""Free precession with explicit Jacobian operation (backward)

    This operator is only differentiable w.r.t. ``Mi``.

    """

    @staticmethod
    def forward(
        ctx: CTX, Mi: Tensor, dur: Tensor,
        T1: Optional[Tensor], T2: Optional[Tensor], Œîf: Optional[Tensor]
    ) -> Tensor:
        r"""Forward operation of free precession

        Inputs:
            - ``ctx``: `(1,)`, pytorch CTX cacheing object
            - ``Mi``: `(N, *Nd, xyz)`, Magnetic spins, assumed equilibrium \
              [0 0 1]
            - ``dur``: `(N ‚äª 1, len(Nd)*(1,), 1)`, "Sec", dwell time.
            - ``T1``: `(N ‚äª 1, *Nd ‚äª len(Nd)*(1,), 1)`, "Sec", T‚ÇÅ
            - ``T2``: `(N ‚äª 1, *Nd ‚äª len(Nd)*(1,), 1)`, "Sec", T‚ÇÇ
            - ``Œîf``: `(N ‚äª 1, *Nd ‚äª len(Nd)*(1,))`, "Sec", T‚ÇÇ
        Outputs:
            - ``Mo``: `(N, *Nd, xyz)`, Magetic spins after simulation.
        """  # could we learn to live right.

        Mo = Mi.clone(memory_format=_contiguous_format)

        # Precession
        cœï = sœï = tmp = None
        if Œîf is not None:  # positive Œîf dephases clock-wise/negatively
            sœï = -(2*œÄ)*Œîf*dur[..., 0]
            cœï = torch.cos(sœï)
            sœï.sin_()  # œï is now sœï

            tmp = Mo[..., 0].clone(memory_format=_contiguous_format)  # Mix
            Mo[..., 0].mul_(cœï)  # cœï*Mix
            torch.addcmul(Mo[..., 0], sœï, Mo[..., 1], value=-1,
                          out=Mo[..., 0])  # Mox = cœï*Mix - sœï*Miy

            Mo[..., 1].mul_(cœï)
            torch.addcmul(Mo[..., 1], sœï, tmp,
                          out=Mo[..., 1])  # Moy = sœï*Mix + cœï*Miy

        # Relaxation
        E1 = E2 = E1_1 = None
        assert((T1 is None) == (T2 is None))  # both or neither

        if T1 is not None:
            E1, E2 = -dur/T1, -dur/T2
            E1_1 = torch.expm1(E1)  # E1 - 1
            E1.exp_(), E2.exp_()  # should have fewer alloc than exp(-dt/T1)
            Mo[..., 0:2].mul_(E2)
            Mo[..., 2:3].mul_(E1).sub_(E1_1)

        ctx.save_for_backward(cœï, sœï, E1, E2, tmp)

        return Mo

    @staticmethod
    def backward(
        ctx: CTX, grad_Mo: Tensor
    ) -> Tuple[Tensor, None, None, None, None]:
        r"""Backward operation of free precession

        Inputs:
            - ``ctx``: `(1,)`, pytorch CTX cacheing object
            - ``grad_Mo``: `(N, *Nd, xyz)`, derivative w.r.t. output Magetic \
              spins.
        Outputs:
            - ``grad_Mi``: `(N, *Nd, xyz)`, derivative w.r.t. input Magetic \
              spins.
            - None*4, this implemendation do not provide derivatives w.r.t.: \
              `dur`, `T1`, `T2`, and `Œîf`.
        """  # If we turn back time,
        # grads of configuration variables are not supported yet
        needs_grad = ctx.needs_input_grad
        grad_Mi = grad_dur = grad_T1 = grad_T2 = grad_Œîf = None

        if not any(needs_grad[0:1]):
            return grad_Mi, grad_dur, grad_T1, grad_T2, grad_Œîf

        grad_Mi = grad_Mo.clone(memory_format=_contiguous_format)

        # ctx.save_for_backward(cœï, sœï, E1, E2, E1_1)
        cœï, sœï, E1, E2, tmp = ctx.saved_tensors

        # Relaxation
        if E1 is not None:
            grad_Mi[..., 0:2].mul_(E2)
            grad_Mi[..., 2:3].mul_(E1)

        # Precession
        if tmp is not None:
            tmp.copy_(grad_Mi[..., 1])  # copy of ‚àÇMoy
            grad_Mi[..., 1].mul_(cœï)  # cœï*‚àÇMoy
            torch.addcmul(grad_Mi[..., 1], sœï, grad_Mi[..., 0], value=-1,
                          out=grad_Mi[..., 1])  # ‚àÇMiy = -sœï‚àÇMox + cœï*‚àÇMoy

            grad_Mi[..., 0].mul_(cœï)  # cœï*‚àÇMox
            torch.addcmul(grad_Mi[..., 0], sœï, tmp,
                          out=grad_Mi[..., 0])  # ‚àÇMix = cœï*‚àÇMox + sœï‚àÇMoy

        return grad_Mi, grad_dur, grad_T1, grad_T2, grad_Œîf


def freeprec(
    Mi: Tensor, dur: Tensor, *,
    T1: Optional[Tensor] = None, T2: Optional[Tensor] = None,
    Œîf: Optional[Tensor] = None
) -> Tensor:
    r"""Isochromats free precession with given relaxation and off-resonance

    This function is only differentiable w.r.t. ``Mi``.

    Setting `T1=T2=None` to opt for simulation ignoring relaxation.

    Usage:
        ``Mo = freeprec(Mi, dur, *, T1, T2, Œîf)``
    Inputs:
        - ``Mi``: `(N, *Nd, xyz)`, Magnetic spins, assumed equilibrium \
          magnitude [0 0 1]
        - ``dur``: `()` ‚äª `(N ‚äª 1,)`, "Sec", duration of free-precession.
    OPTIONALS:
        - ``T1``: `()` ‚äª `(N ‚äª 1, *Nd ‚äª 1,)`, "Sec", T1 relaxation.
        - ``T2``: `()` ‚äª `(N ‚äª 1, *Nd ‚äª 1,)`, "Sec", T2 relaxation.
        - ``Œîf``: `(N ‚äª 1, *Nd ‚äª 1,)`, "Hz", off-resonance.
    Outputs:
        - ``Mo``: `(N, *Nd, xyz)`, Result magnetic spins
    """
    ndim = Mi.ndim  # dur, T1, T2, Œîf are reshaped to be compatible w/ M
    dur = dur.reshape(dur.shape+(ndim-dur.ndim)*(1,))

    assert((T1 is None) == (T2 is None))  # both or neither
    if T1 is not None:  # ‚Üí (N ‚äª 1, *Nd ‚äª len(Nd)*(1,), 1)
        T1, T2 = (x.reshape(x.shape+(ndim-x.ndim)*(1,)) for x in (T1, T2))

    if Œîf is not None:  # ‚Üí (N ‚äª 1, *Nd ‚äª len(Nd)*(1,))
        Œîf = Œîf.reshape(Œîf.shape+(ndim-1-Œîf.ndim)*(1,))

    return FreePrec.apply(Mi, dur, T1, T2, Œîf)
